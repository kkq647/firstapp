# -*- coding: utf-8 -*-
"""predictive analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iRmTUMOsKba47YDMlUtY4ZfUbv1Tr7R9
"""

from sklearn.model_selection import train_test_split
from sklearn.impute import MissingIndicator
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from xgboost import XGBClassifier


"""# Our Own Method

在检查了hw里处理的方法所得到的结果后，因为考虑到我们最终的成果是要呈现一个Decision Support System（DSS），如果要求使用者填入23个features显然很麻烦也不够实用，所以我们的思路是挑选重要的features，这也是为什么我们在上一步要使用XGBoost。同时，对于selected features我们要再次检查里面是否有missing values，并考虑换一种适当的方式处理它们。

## Load Data
"""

new_names = ['RP']
factor = 'F'
for i in range(1,24):
   name = factor + str(i)
   new_names.append(name)

missing_values = ['-7', '-8', '-9']
df = pd.read_csv('/Users/liuxiaoqi/Downloads/heloc_dataset_v1.csv', na_values = missing_values, names = new_names, header = 0)
df.isnull().sum()

"""## Check Correlation Between Features and Risk Performance"""

sns.set(style='white',context='notebook',palette='muted')

df = df.replace('Good',0)
df = df.replace('Bad',1)

plt.figure(figsize=(24,24))
sns.heatmap(df[df.columns].corr(),cmap='BrBG',annot=True,
           linewidths=.5)
plt.xticks(rotation=45)

"""We consider that correlation below 0.1 is low, according to the heat map, F3, F13, F19, and F21 have low correlation with the 'RP', which is our target, Risk Performance.

基于features importances的结果，我们挑选前八个最重要的features来训练model，在上面这里我们又进一步检查了每个features和Y的相关性，显然我们要选择的8个features都不在low correlation之列，后面就可以放心的筛选出我们要的features了

## Select Features
"""

df = df[['RP', 'F1', 'F2', 'F4', 'F5', 'F12', 'F14', 'F18', 'F23']]
df.isnull().sum()

"""我们看到筛选出8个features之后，这8个features所含的missing values都不多，并且我们之前已经知道了那588个是共有的，相对于整个data set的大小来说直接drop影响不大

## Data Preprocess （1） - Drop
"""

df = df.dropna(axis=0,subset=['F1'])
df.isnull().sum()

"""这一步drop掉了共同缺失的rows，下一步我们用mean来填补剩下的missing values。Why用mean？这个随便搜一下用mean填补missing values的好处的答案然后往我们数据集的实际情况和每个features的实际意义上扯一下吧

## Data Preprocess (2) - Mean Fill
"""

df['F2'].fillna(df['F2'].mean(), inplace=True)
df['F18'].fillna(df['F18'].mean(), inplace=True)
df['F23'].fillna(df['F23'].mean(), inplace=True)
df.isnull().sum()

df

"""## Split Train Set, Test Set and Validation Set"""

X = df.iloc[:,1:9]
Y = df.iloc[:, 0].replace({"Bad": 1, "Good": 0})

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4321)

X_train_tr, X_train_val, Y_train_tr, Y_train_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=4321)

"""## Train Model & Training Accuracy

对于我们处理好的data set，再search一下optimal parameters
"""

params = {
    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],
    'learning_rate': np.linspace(0.01, 0.5, 50),
    'subsample': np.linspace(0.5, 1, 50),
    'colsample_bytree': np.linspace(0.5, 1, 50),
    'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    'reg_lambda': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]
}


clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc')

random_search = RandomizedSearchCV(
    estimator=clf, 
    param_distributions=params, 
    n_iter=100,
    scoring='roc_auc',
    n_jobs=-1,
    cv=5,
    verbose=1, 
    random_state=1
)


random_search.fit(X_train_tr, Y_train_tr)

print("Best parameters:", random_search.best_params_)
print("Best score:", random_search.best_score_)

params = {
    'max_depth': 4,
    'learning_rate': 0.02,
    'subsample': 0.5102040816326531,
    'reg_lambda': 0.01, 
    'reg_alpha': 1,
    'gamma': 0.001,
    'colsample_bytree': 0.7244897959183674,
    'objective': 'binary:logistic',
    'eval_metric': 'auc'
}

model = XGBClassifier(**params)
model.fit(X_train_tr, Y_train_tr)

training_accuracy_xgb = accuracy_score(Y_test, model.predict(X_test))
print('Accuracy: %.2f%%' % (training_accuracy_xgb * 100.0))

"""## Validation Accuracy"""

val_accuracy_xgb = accuracy_score(Y_train_val, model.predict(X_train_val))
print('Accuracy: %.2f%%' % (val_accuracy_xgb * 100.0))

"""# Interactive Interface Part

大概的设想是要求8个输入，在界面上显示的是我们上面那8个Features的名字，并且我们要求的是这8个值不允许有缺失值（这样会在代码实现上简单点），然后这8个input接收后把它们放进我们的模型预测，给出一个prediction，0是good， 1是bad（这部分有时间可以把output转码成一句话or转码回good or bad）
"""

import pickle
with open('model.p','wb') as f:
    pickle.dump(model,f)

print('done!')

